from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

server = "ip:port" # Host do sql server
database = "DB NAME" # Nome do database
username = " " # Autenticação para se conectar ao sql server
password = " " # Autenticação para se conectar ao sql server

jdbc_url = f"jdbc:sqlserver://{server};databaseName={database};encrypt=false"

connection_properties = {
    "user": username,
    "password": password,
    "driver": "com.microsoft.sqlserver.jdbc.SQLServerDriver"
}

query_tables = """
(
SELECT TABLE_SCHEMA, TABLE_NAME
FROM INFORMATION_SCHEMA.TABLES
WHERE TABLE_TYPE = 'BASE TABLE' AND TABLE_SCHEMA <> 'dbo'
) AS table_list
"""

df_tables = spark.read.jdbc(
    url=jdbc_url,
    table=query_tables,
    properties=connection_properties
).collect()

def save_single_jsonl(df, output_path):
    temp_dir = output_path + "_temp_dir"

    df.coalesce(1).write.mode("overwrite").json(temp_dir)

    files = dbutils.fs.ls(temp_dir)
    part_file = [f.path for f in files if f.name.startswith("part-") and f.name.endswith(".json")]

    if not part_file:
        raise FileNotFoundError(f"Nenhum arquivo part-*.json encontrado em {temp_dir}")

    part_file = part_file[0]

    dbutils.fs.mv(part_file, output_path)
    dbutils.fs.rm(temp_dir, recurse=True)

for row in df_tables:
    schema = row["TABLE_SCHEMA"]
    table = row["TABLE_NAME"]
    full_table = f"{schema}.{table}"

    print(f" Extraindo {full_table}...")

    try:
        df = spark.read.jdbc(
            url=jdbc_url,
            table=f"(SELECT * FROM {full_table}) AS RAW_{schema}_{table}",
            properties=connection_properties
        )

        output_file = f"/Volumes/adventureworks/ingest/raw/RAW_{schema}_{table}.jsonl"

        save_single_jsonl(df, output_file)

        print(f" {full_table} salvo em: {output_file}")

    except Exception as e:
        print(f" Erro ao extrair {full_table}: {e}")
