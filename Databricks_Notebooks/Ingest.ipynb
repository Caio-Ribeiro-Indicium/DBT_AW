import os

volume_path = "CAMINHO DOS ARQUIVOS" #Caminho para os arquivos baixados do SQL server
file_names = [f for f in os.listdir(volume_path) if f.endswith('.jsonl')] #Buscar somente arquivos terminados em .jsonl (caso suba algo por engano, evita erros)

for file_name in file_names: #Para cada arquivo nos arquivos .jsonl encontrados
    file_path = f"{volume_path}/{file_name}" #Caminho dos arquivos + nome do arquivo
    table_name = file_name.replace(".jsonl", "") #Nome da tabela = remove o ".jsonl"     
    print(f"Reading: {file_path}") #Imprime arquivo que está sendo lido
    df = spark.read.json(file_path) #Lê arquivo

    print(f"Creating Delta table: adventureworks/ingest/.{table_name}") #Imprime nome da tabela que será criada
    df.write.format("delta") \
        .mode("overwrite") \
        .option("mergeSchema", "true") \
        .saveAsTable(f"adventureworks.ingest.{table_name}") #Escreve tabela em modo overwrite (sobreescreve tabela se já houver com mesmo nome) e em modo mergeSchema (Adiciona colunas novas caso haja, evita erros)