
# Projeto DBT_AW AdventureWorks Pipeline

**VersÃ£o 1.1**

Este repositÃ³rio contÃ©m um pipeline completo de extraÃ§Ã£o, ingestÃ£o e transformaÃ§Ã£o da base de dados *AdventureWorks*.

## ğŸ§  VisÃ£o Geral

Este projeto tem como objetivo:

- Extrair dados da base *AdventureWorks* a partir de um banco **Microsoft SQL Server**
- Salvar os dados no **Databricks** em arquivos **JSONL**
- Ingerir esses arquivos como **Delta Tables** no Databricks
- Transformar os dados usando **DBT Cloud**, nas camadas:
  - "stg" (staging)
  - "int" (intermediÃ¡ria)
  - "marts" (dimensÃµes e fatos)
- Gerar as tabelas finais:
  - "dim_clientes"
  - "dim_producao_produto"
  - "dim_produto"
  - "dim_vendedor"
  - "fato_producao"
  - "fato_vendas"

>  Este projeto Ã© compatÃ­vel com as versÃµes gratuitas do **Databricks** e **DBT Cloud**.

---

##  PrÃ©-requisitos

- Conta ativa no Databricks (https://community.cloud.databricks.com/)
- Conta gratuita no DBT Cloud (https://www.getdbt.com/signup/)
- Acesso a um banco Microsoft SQL Server com a base AdventureWorks
- Token de acesso pessoal no Databricks (para uso via API ou notebook)
- Git instalado localmente (para clonar o repositÃ³rio)

---

## ğŸš€ Passo a Passo para Executar o Projeto

### ğŸ” 1. Clonar e importar repositÃ³rio
- Clone este repositÃ³rio:
  """bash
  git clone https://github.com/Caio-Ribeiro-Indicium/DBT_AW.git
  """
- No DBT Cloud, vÃ¡ atÃ© a aba **"Develop" > "Start from Repo"** e importe o repositÃ³rio GitHub.

### âš™ï¸ 2. Configurar ambiente e job no DBT Cloud
- Em **"Deploy" > "Environments"**, crie um novo *Environment* (ex: "prod")
- Em **"Deploy" > "Jobs"**, crie um *Job* com o comando:
  """
  dbt build
  """

### ğŸ““ 3. Configurar Notebooks no Databricks
- No Databricks, importe os notebooks do diretÃ³rio "Databricks_Notebooks":
  - "01_EXTRACT.sql"
  - "02_INGEST.sql"
  - "03_CALL_DBT_API.py"
- Acesse a aba **Workspace > Import** e selecione os arquivos ".sql" ou ".py".

### ğŸ—‚ï¸ 4. Criar estrutura de destino no Databricks
- Crie um **catalog** chamado "adventureworks"  
- Dentro dele, crie um **schema** chamado "ingest"
- Dentro do schema, crie um **volume** chamado "raw"

> âš ï¸ Esses nomes sÃ£o opcionais, mas caso altere, vocÃª precisarÃ¡ atualizar os caminhos nos notebooks.

### ğŸ”‘ 5. Preencher credenciais nos notebooks
- Abra os notebooks e preencha as variÃ¡veis com:
  - URLs do SQL Server
  - Credenciais de conexÃ£o
  - Token e host do Databricks
- Todos os trechos estÃ£o comentados para facilitar o preenchimento.

### â±ï¸ 6. Criar Job de orquestraÃ§Ã£o no Databricks
- VÃ¡ atÃ© **"Jobs & Pipelines" > "Create Job"**
- Adicione os notebooks na seguinte ordem:
  1. "EXTRACT"
  2. "INGEST"
  3. "CALL_DBT_API"

---

## ğŸ“ Estrutura do RepositÃ³rio

"""
DBT_AW/
â”‚
â”œâ”€â”€ Databricks_Notebooks/
â”‚   â”œâ”€â”€ 01_EXTRACT.ipynb
â”‚   â”œâ”€â”€ 02_INGEST.ipynb
â”‚   â””â”€â”€ 03_CALL_DBT_API.ipynb
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ stg/        â†’ Camada de staging
â”‚   â”œâ”€â”€ int/        â†’ Camada intermediÃ¡ria
â”‚   â””â”€â”€ marts/      â†’ Fatos e dimensÃµes
â”‚
â”œâ”€â”€ dbt_project.yml â†’ ConfiguraÃ§Ãµes do projeto DBT
â””â”€â”€ README.md       â†’ Este arquivo
